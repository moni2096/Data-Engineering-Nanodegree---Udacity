# Introduction
A startup called Sparkify wants to analyze the data they have been collecting on their music streaming app. The data resides in JSON fomat on the local drive and as a part of setting of their analytics workflow they are looking to set up a Postgres database to run the optimized queries on the song play analysis.

# Goal
The goal of this project is to create a simple ETL pipeline by using Data Modeling with Postgres in Python. The pipeline reads the JSON data from the local drive and stores it into the modeled tables.

# Data Sources
The datasets used in this projects are divided into two different parts:
1) Song Data: The data is a subset of real data from [Million Song Dataset](http://millionsongdataset.com/pages/getting-dataset/#subset). The files are in JSON formaat and contains metadata about song and artist of the table.
2) Log Data: The second dataset consists of log data in JSON format generated by this [event simulator](https://github.com/Interana/eventsim).

# Data Processing and ETL 
The data modeling phase started with setting up the star schema and dividing the required tables into fact table and dimension tables since we needed optimized queries for song play analysis. The schema of the tables look as shown below:
1) Fact Table: 
* Table Name: songplays - records in log data associated with song plays i.e records with Page Next Song in the log data files.
* Table Schema: The table schema is represented below
    | Column Name   | Data Type     |
    | ------------- | ------------- |
    | songplay_id   | SERIAL (PRIMARY KEY) |
    | start_time    | BIG INT  |
    | user_id       | INTEGER  |
    | level         | VARCHAR   |
    | artist_id     | VARCHAR  |
    | session_id    | INTEGER  |
    | location      | VARCHAR  |
    | user_agent    | VARCHAR |

2) Dimenision Tables:
* Table Name: users-users of the app Sparkify
* Table Schema: The table schema is represented below
    | Column Name   | Data Type     |
    | ------------- | ------------- |
    | user_id       | INTEGER (PRIMARY KEY) |
    | first_name    |  VARCHAR  |
    | last_name     | VARCHAR  |
    | gender        | VARCHAR   |
    | level         | VARCHAR  |

* Table Name: songs-songs in the music database of the application
* Table Schema: The table schema is represented below
    | Column Name   | Data Type     |
    | ------------- | ------------- |
    | song_id       | INTEGER (PRIMARY KEY) |
    | title         |  VARCHAR  |
    | artist_id     | VARCHAR  |
    | year        | INTEGER   |
    | duration    | FLOAT8  |

* Table Name: artists-artist in music database of application
* Table Schema: The table schema is represented below
    | Column Name   | Data Type     |
    | ------------- | ------------- |
    | artist_id     | VARCHAR (PRIMARY KEY)  |
    | name         |  VARCHAR  |
    | location     | VARCHAR  |
    | latitude     | FLOAT8   |
    | lngitude     | FLOAT8  |

* Table Name: time - timestamp of the record in songplays table broken down into specific units
* Table Schema: The table schema is represented below
    | Column Name   | Data Type     |
    | ------------- | ------------- |
    | start_time     | TIMESTAMP (PRIMARY KEY)  |
    | hour          |  INTEGER  |
    | day           | INTEGER  |
    | week         | INTEGER   |
    | month       | INTEGER  |
    | year        |  INTEGER |
    | weekday     | INTEGER  |

# Project Guidelines
* **data** - The directory contains data divided into two folders one contains the song_data and the other one contains log_data.
* **sql_queries.py** -  The file contains sql queries that are used to model the fact and dimension table mentioned in the above section.
* **create_tables.py** - The file contains script to connect to the Postgres database and model the table using queries in sql_queries.py.
* **etl.ipynb** - The notebook contains the demonstration of ETL process used to transform data from raw json files to modeled tables in Postgres database.
* **etl&#46;py** - The file contains the entire ETL code packaged in functions to transform data into tables.
* **test.ipynb** - The notebooks contains simple test comands to query some of the rows of each modeled table to ensure the data flows in correctly.

* To run this project make sure to run create_tables.py before running any other file using below command
    ``` 
        python create_tables.py
    ```
* If you want to look at the demonstartion of entire ETL process make sure to look into etl.ipynb using Jupyter Notebook launcher. The best way to download Jupyter Notebook is by using [Anaconda Installers](https://www.anaconda.com/products/individual).
* To model entire data into tables run etl&#46;py using below command
    ``` 
       python etl.py
    ```
* Finally, don't forget to test the data in modeled tables using test.ipynb.







    




