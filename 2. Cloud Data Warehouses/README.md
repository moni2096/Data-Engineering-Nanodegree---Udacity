# Introduction
A startup called Sparkify wants to analyze the data they have been collecting on their music streaming app. The data resides in S3 in JSON fomat and with growing user base they are trying to create a data warehouse in AWS.

# Goal
The goal of this project is to create a data warehouse by choosing appropriate schema for modeling the tables in resdhift database with and ability to query the modeled table.

# Data Sources
The datasets used in this projects are divided into two different parts:
1) Song Data: The data is a subset of real data from [Million Song Dataset](http://millionsongdataset.com/pages/getting-dataset/#subset). The files are in JSON formaat and contains metadata about song and artist of the table.
2) Log Data: The second dataset consists of log data in JSON format generated by this [event simulator](https://github.com/Interana/eventsim).

# Schema Design
* To load the data from S3 into database we will first create two staging tables:
	* staging_events: To stage events data.
	* staging_songs: To stage song data.
* For the sake of analysis we will create a star schema and load data into facts and dimension table from the above created staging tables
	* The fact table will be:
		*	songplays: records in event data associated with song plays
	* The dimension table will be:
		* users-users of the app Sparkify
		* songs-songs in the music database of the application
		* artists-artist in music database of application
		* time-timestamp of the record in songplays table broken down into specific units
* For detailed design of data types of these tables and primary key please check [here](Link to read me for project 1)
# Project Guidelines
* **dwh.cfg**- The configuration file to store and fetch credentials of AWS components.
* **create_cluster.py** - Used to create cluster, creatr IAM role and attach policy.
* **delete_cluster.py** - To destroy cluster and destroy AWS components.
* **sql_queries.py** - Used to model the staging, fact and dimension tables mentioned in the above section.
* **etl&#46;py** - Used to load raw data from S3 into staging, facts and dimension tables.
* **analysis_test.ipynb**- Used to test queries and analyze the data from modeled tables.

# Steps to run this Project
#### Step 1
Creat an IAM role by going to [AWS](https://aws.amazon.com/). The guide for various components inside AWS can be found [here](https://www.dropbox.com/s/ql8wxqjjcv42065/aws-components-setup.pdf?dl=0)

#### Step 2
Create a Security Group by using the same guide linked above.

#### Step 3
Update the dwh.cfg file with AWS Key and AWS secret.

#### Step 4
Create a redshift cluster. This can be done by using the GUI in the guide linked above or running the python file create_cluster.py

`` python create_cluster.py``

Note: This file will produce error untill the cluster status becomes available. Please check the cluster status by logging into AWS account and run the file again to get the enpoint.

#### Step 5
If you have created cluster using python file with above command it will return data warehouse end point (DWH_END_POINT) and data warehouse role arn (DWH_ROLE_ARN). Please update dwh.cfg file with the host end point of cluster and IAM role ARN. The dwh.cfg file should contain all the details that are not mentioned.

#### Step 6
If the AWS components are created run the below command to create tables by connecting to Postgres database in redshift cluster.
``python create_tables.py``

#### Step 7
Run the below command to load the staging tables as well as facts and dimension tables described above using the files stored in S3.
``python etl.py``

#### Step 8
After loading all the tables run analysis_test.ipynb using Jupyter Notebook Launcher to check whether the tables are loaded properly.

#### Step 9 
Finally dont forget to destroy your cluster to avoid additional charges in future.
``python destory_cluster.py``